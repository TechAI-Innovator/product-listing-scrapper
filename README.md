# ğŸ“˜ Book Scraper CLI â€” Python Automation Engineer Take-Home

A modular web scraper built with **Playwright** that extracts product data from [books.toscrape.com](https://books.toscrape.com), simulates human browsing behavior, and saves the results in your chosen format.

---

## ğŸ¯ Features

- âœ… Asynchronous scraping with Playwright
- âœ… CLI support with customizable options
- âœ… Human-like behavior: random delays, user-agent rotation
- âœ… Reuses browser sessions/cookies
- âœ… Supports saving to JSON, CSV, or SQLite (via SQLAlchemy)
- âœ… Graceful error handling and retry logic
- âœ… Modular, testable, and scalable code structure

---

## ğŸ› ï¸ Setup

```bash
# Create a virtual environment (optional but recommended)
python -m venv venv
source venv/bin/activate  # or venv\Scripts\activate on Windows

# Install dependencies
pip install -r requirements.txt

# Install browser binaries for Playwright
playwright install
```

---

## ğŸš€ Usage

Run the CLI tool with desired options:

```bash
python scraper/cli.py --max-pages 3 --output-format csv --headless --reuse-session
```
## ğŸ³ Docker

A working `Dockerfile` is included and tested in a Linux-compatible environment. It installs Playwright, its browser dependencies, and runs the scraper with default CLI arguments.

> âš ï¸ Docker build on Windows (especially with Playwright) can be slow or problematic due to system dependencies and WSL configuration. For this reason, the container was not tested locally, but the configuration is complete and expected to work on Linux or cloud CI systems.

To build and run (if supported):

```bash
docker build -t book-scraper .
docker run --rm book-scraper
```

To override CLI options:
```bash
docker run --rm book-scraper python scraper/cli.py --max-pages 5 --output-format csv
```

### CLI Options

| Argument         | Description                                         | Default    |
|------------------|-----------------------------------------------------|------------|
| `--max-pages`    | Max number of pages to scrape                       | `3`        |
| `--output-format`| Output type: `json`, `csv`, or `sqlite`             | `json`     |
| `--headless`     | Run browser in headless mode                        | `True`     |
| `--reuse-session`| Reuse cookies/session state from previous run       | `False`    |

---

## ğŸ“‚ Project Structure

```
scraper/
â”œâ”€â”€ cli.py           # CLI interface and entry point
â”œâ”€â”€ scraper.py       # Core scraping logic
â”œâ”€â”€ storage.py       # Save to JSON, CSV, or SQLite
â”œâ”€â”€ utils.py         # Helpers (delays, star rating parsing)
â”œâ”€â”€ config.py        # Constants like BASE_URL, user-agents
â”œâ”€â”€ parsers.py       # Rating parser from CSS classes
requirements.txt
README.md
output.json / output.csv / books.db
```

---

## ğŸ“¦ Sample Outputs

- `output_20250624_1450.json`
- `output_20250624_1451.csv`
- `books_20250624_1452.db`

---

## ğŸ§ª Running Tests

```bash
pytest tests/
```

## ğŸ§ª Optional Enhancements (Not Implemented)

- Dockerfile for containerized execution
- Proxy rotation (e.g., ProxyMesh)

---


## ğŸ§‘â€ğŸ’» Author

**NAME: Ayomide Adu**  
**EMAIL: techai.innovator@gmail.com**
**GITHUB: https://github.com/techai-innovator**
